\subsubsection{Partially Observable Markov Decision Processes}

A Partially Observable Markov Decision Process (POMDP) is a further extension of a Markov Decision Processe, the difference being that the decision maker can no longer observe the entire system state, but must instead deal with partial observations when making decisions. It is formally defined as a six-tuple $(S,A,O,T,\Omega,R)$ with:
\begin{itemize}
\item $S$: set of states,
\item $A$: set of actions,
\item $O$: set of observations,
\item $T$: conditional transition probabilities,
\item $\Omega$: conditional observation probabilities,
\item $R$: rewards.
\end{itemize}

\subsubsubsection{Observations}

An observation is any system output that the decision maker can \textit{observe}. MDPs assume that the decision maker has the ability to \textit{see} what state the system is currently in, whereas POMDPs make no such assumption, relying instead on partial observations.

\subsubsubsection{Optimization}

Although the reward function for MDPs and POMDPs remains the same, optimization differs in that the output is no longer a policy that maps \textit{states} to \textit{actions}, but rather a policy that maps \textit{observations} to \textit{actions}. This more realistically models reality where decision makers often do not have the ability to observe the complete state of a system, but rather receive only partial information (example: sensor data). The result of this added observation dimension is that the decision agent must now maintain a constantly changing belief state, that is initially defined and then continuously updated as actions are taken and observations received.

The computation of optimal policies for POMDPs is computationally extremely costly. The traditional policy computation approaches used for MDPs are often intractable for POMDPs and it is thus often necessary to solve approximately. A number of interesting approaches have been studied \cite{Hansen98}.