\subsection{Partially Observable Markov Decision Processes}
\label{subsec:pomdp}


A Partially Observable Markov Decision Process (POMDP) is a further extension of a Markov Decision Process, the difference being that the decision maker can no longer perfectly observe the entire system state, but must instead deal with partial observations when making decisions. It is formally defined as a six-tuple $(S,A,O,T,\Omega,R)$ with:
\begin{itemize}
\item $S$: set of states,
\item $A$: set of actions,
\item $O$: set of observations,
\item $T$: conditional transition probabilities,
\item $\Omega$: conditional observation probabilities,
\item $R$: rewards.
\end{itemize}

An observation is any system output that the decision maker can \textit{observe}. MDPs assume that the decision maker has the ability to \textit{see} what state the system is currently in, whereas POMDPs make no such assumption, relying instead on partial observations. This approach more realistically models reality where systems are rarely completely observable. The notion of actions and rewards remains the same with POMDPs. 

\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
                    semithick,every text node part/.style={align=center}]
  \tikzstyle{block}=[draw,rectangle,text=black]

  \node [block] (policy) at (0.7,-0.7) {Policy \\ $\pi: b(s) \rightarrow A$};

  \draw[->,>=stealth',semithick] (-1.5,-0) arc[radius=2, start angle=90, end angle=360];

  \node at (-5,-1) {1). take action $a_t$};
  \node at (-1,-5) {2). update belief state $b_{t+1}(s)$};
  \node at (3,-2.8) {3). now in belief state $b_{t+1}$};
  \node at (1.2,1) {4). get action $a_{t+1}$ for belief\\ state $b_{t+1}$ from policy};
  
\end{tikzpicture}
\end{center}
\caption{POMDP control loop (offline planning)}
\label{pomdpcontrolbasic}
\end{figure}

Optimization using POMDPs is an order of magniture more complicated than with MDPs \cite{mdpai}\cite{littman96}. The product of an optimal planning procedure is no longer a policy $\pi: S \rightarrow A$, but a policy $\pi: P(S) \rightarrow A$, that maps probability distributions over the state-space to actions. Because exact knowledge of the system's state is no longer available, the controller must maintain a belief-state, a probability distribution over the state-space, representing the probabilities of the system currently being in a certain state. Figure~\ref{pomdpcontrolbasic} shows the traditional control loop of an abstract POMDP based controller. After action $a_t$ is taken at time $t$ the controller must update his belief-state for time $t+1$, meaning he must update the distribution of probabilities that the system will be in a given state $s_{t+1}$ at time $t+1$. The update makes use of both the transition probabilities $T(s_{t+1}|s_t,a_t)$ and the observation $o_{t+1}$ the controller receives at time {t+1}. The belief-state at time $t+1$,
% \[
% b_{t+1}(s) = \frac{1}{\sum_{s_{t+1} \in S} \Omega (o_t|s_{t+1},a_t) \sum_{s_t \in S} T(s_{t+1}|s_t,a_t)\cdot b(s_t)} \cdot \Omega (o_t|s_{t+1},a_t) \cdot \sum_{s \in S} T(s_{t+1}|s_t,a_t) \cdot b(s_t),
% \]
\[
b_{t+1}(s) = F(o_t,a_t,b_t(s)),
\]

is then used to query the policy for the action $a_{t+1}$ to take at time $t+1$ (function $F$ is defined in \cite{mdpai}). It is immediately obvious that the policy produced by a POMDP planner is now 3-dimensional, as opposed to the 2-dimensional policy an MDP produces. Instead of mapping from finite \textit{states} to \textit{actions} the POMDP policy must map \textit{distributions over states} (ie. belief states) to actions. This adds an enormous computational cost to planning using POMDPs. Solving POMDPs (ie. producing a policy) is often intractable because of the amount of necessary calculations. Therefore much research has been done with alternative, mostly approximative, methods \cite{Hansen98}\cite{littman96}.

The above cost is based on the fact that the policy must be able to map all possible belief-states to actions. In order to overcome this computational cost POMDPs are sometimes used for \textit{online planning}. In this case the policy is not computed in advance for all possible belief states, but rather computed at each time step for a single belief state, the one the system is in when the controller must make a decision. This approach is introduced in \cite{mdpai}, covered in detail in \cite{ross2008online} and unfortunately outside the scope of this text.

Although POMDP control is associated with high computational costs it is nonetheless an approach with merit. The distinction between system state and observations reflects the reality of most complex systems and the resulting policies reflect this realism by taking into account that what is observed may differ from what is.

