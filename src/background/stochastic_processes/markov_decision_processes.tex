\subsection{Markov Decision Processes}

Markov Decision Processes (MDPs) are an extension of Markov Chains and describe \textit{controllable} probabilistic dynamic systems. They are defined as a four touple $(S,A,P,R)$ with:
\begin{itemize}
\item $S$: set of states,
\item $A$: set of actions,
\item $P$: conditional transition probabilities,
\item $R$: rewards.
\end{itemize}

MDPs are used to model probabilistic systems that can be influenced through decision-taking. These decisions are represented as actions and have a direct influence on the transition probabilities of the system. Transition probabilities are now three-dimensional and depend not only on the current state, but also on the action being taken; $Pr(X_{n+1}=x_n|X_n=x,a_n=a)$ is the probability of going to state $x_n$ in the next step given that the system is currently in state $x$ and action $a$ has been chosen.

MDPs also define rewards. Although rewards are not necessary to describe \textit{controllable} dynamic systems, they are necessary when using MDPs for optimization. Every transition probability is paired with a reward, or cost (negative reward), value; $R(s,s',a)$ is the reward (scalar) that the system receives when it transitions from state $s'$ to state $s$ given that action $a$ was chosen. \textit{Note that rewards are not inherently probabilistic}.

MDPs can be used to optimize decision making. The combination of a system description and an associated reward model allows the computation on an optimal decision to take in a given situation. The aim of this optimization is to produce a \textit{policy}, $\pi(s)$, that defines exactly which action should be taken if the system finds itself in a certain state.

The computation of an optimal policy requires the definition of \textit{optimality}. In most cases optimization simply aims for the maximization of rewards (or minimization of costs) over a certain decision span. This optimization goal is defined in a so called reward function, the most common of which is the \textit{expected discounted total reward} (infinite horizon),

\[
\sum_{t=0}^{\infty} {\gamma}^{t}R_{a_t}(s_t,s_{t+1}),
\]

with:
\begin{itemize}
\item $\gamma$: discount factor, where $\gamma\in(0,1]$,
\item $R_{a_t}(s_t,s_{t+1})$: reward received in $t+1$ for taking action $a$ from state $s_t$ at time $t$.	
\end{itemize}

The \textit{expected discounted total reward} seen above is only one of many possible reward functions. It represents the idea that the decision maker values the total sum of rewards, but prefers rewards received earlier to rewards received later, ie. \textit{rewards are discounted over time}.

Given an MDP and a reward function, an optimal policy can be computed. The computation of such a policy can be undertaken using either \textit{linear programming} or, more commonly, \textit{dynamic programming}, namely value iteration. An in-depth analysis of the different policy computation algorithms is out of the scope of this text, but is covered in detail by the field's literature \cite{puterman}. The result of the policy computation is, as described above, a policy $\pi(s)$ that maps states to actions and thus provides a decision maker with an \textit{optimal decision} to take when the controlled system finds itself in a certain state.